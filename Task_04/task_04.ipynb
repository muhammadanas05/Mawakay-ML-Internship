{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st-night-time Dataset:\n",
      "   No.      Time          Source     Destination Protocol  Length  \\\n",
      "0    1  0.000000  103.12.198.162      39.62.1.38      UDP    1274   \n",
      "1    2 -0.005975      39.62.1.38  142.250.181.54      TCP      78   \n",
      "2    3 -0.005898      39.62.1.38  103.12.198.146      TCP      66   \n",
      "3    4 -0.005898      39.62.1.38  103.12.198.146      TCP      66   \n",
      "4    5 -0.005889      39.62.1.38  103.12.198.146      TCP      66   \n",
      "\n",
      "                                                Info  \n",
      "0                             443  >  45841 Len=1232  \n",
      "1  2655  >  443 [ACK] Seq=1 Ack=1 Win=3708 Len=0 ...  \n",
      "2  44538  >  443 [ACK] Seq=1 Ack=1 Win=1736 Len=0...  \n",
      "3  44540  >  443 [ACK] Seq=1 Ack=1 Win=1734 Len=0...  \n",
      "4  44540  >  443 [ACK] Seq=1 Ack=2761 Win=1736 Le...  \n",
      "\n",
      "midnight Dataset:\n",
      "   No.      Time          Source Destination Protocol  Length  \\\n",
      "0    1  0.000000    23.63.110.66  39.62.1.38      SSL    1498   \n",
      "1    2  0.000000    23.63.110.66  39.62.1.38      TCP    1494   \n",
      "2    3  0.000013  103.12.198.146  14.2.1.224      SSL    1446   \n",
      "3    4  0.000015  103.12.198.146  14.2.1.224      TCP    1446   \n",
      "4    5  0.000016  103.12.198.146  14.2.1.224      TCP    1454   \n",
      "\n",
      "                                                Info  \n",
      "0                                  Continuation Data  \n",
      "1  [TCP Retransmission] 443  >  37488 [ACK] Seq=1...  \n",
      "2                                  Continuation Data  \n",
      "3  [TCP Retransmission] 443  >  44502 [ACK] Seq=1...  \n",
      "4  [TCP Retransmission] 443  >  44502 [ACK] Seq=1...  \n",
      "\n",
      "saturday aft 1 Dataset:\n",
      "   No.      Time          Source     Destination Protocol  Length  \\\n",
      "0    1  0.000000  103.73.100.205      39.62.1.38      UDP    1292   \n",
      "1    2  0.000052  103.73.100.204      39.62.1.38      UDP    1296   \n",
      "2    3 -0.002708  103.73.103.205      39.62.1.38  TLSv1.2    1498   \n",
      "3    4 -0.002708  103.73.103.205      39.62.1.38      TCP    1494   \n",
      "4    5 -0.002537      14.2.1.239  103.73.103.207      UDP      94   \n",
      "\n",
      "                                                Info  \n",
      "0                             443  >  52072 Len=1250  \n",
      "1                             443  >  62323 Len=1250  \n",
      "2                             Ignored Unknown Record  \n",
      "3  [TCP Retransmission] 443  >  33158 [PSH, ACK] ...  \n",
      "4                               40500  >  443 Len=44  \n",
      "\n",
      "1st-night-time Dataset Columns:\n",
      "Index(['No.', 'Time', 'Source', 'Destination', 'Protocol', 'Length', 'Info'], dtype='object')\n",
      "\n",
      "midnight Dataset Columns:\n",
      "Index(['No.', 'Time', 'Source', 'Destination', 'Protocol', 'Length', 'Info'], dtype='object')\n",
      "\n",
      "saturday aft 1 Dataset Columns:\n",
      "Index(['No.', 'Time', 'Source', 'Destination', 'Protocol', 'Length', 'Info'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "data_1 = pd.read_csv('first-night-time.csv')\n",
    "data_2 = pd.read_csv('midnight.csv')\n",
    "data_3 = pd.read_csv('saturday-aft-1.csv')\n",
    "\n",
    "# Display the first few rows of each dataset\n",
    "print(\"1st-night-time Dataset:\")\n",
    "print(data_1.head())\n",
    "\n",
    "print(\"\\nmidnight Dataset:\")\n",
    "print(data_2.head())\n",
    "\n",
    "print(\"\\nsaturday aft 1 Dataset:\")\n",
    "print(data_3.head())\n",
    "\n",
    "# Display the columns of each dataset\n",
    "print(\"\\n1st-night-time Dataset Columns:\")\n",
    "print(data_1.columns)\n",
    "\n",
    "print(\"\\nmidnight Dataset Columns:\")\n",
    "print(data_2.columns)\n",
    "\n",
    "print(\"\\nsaturday aft 1 Dataset Columns:\")\n",
    "print(data_3.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "data = pd.concat([data_1, data_2, data_3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['No.', 'Info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix and target variable\n",
    "X = data.drop(columns=['Length'])\n",
    "y = data['Length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Identify categorical and numerical columns\n",
    "categorical_features = ['Source', 'Destination', 'Protocol']\n",
    "numerical_features = ['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for categorical data\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing and split data\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training data: [0 0 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution after split\n",
    "class_counts = np.bincount(y_train)\n",
    "print(\"Class distribution in training data:\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance\n",
    "smote = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE cannot be applied. Check data for class imbalance or size issues.\n"
     ]
    }
   ],
   "source": [
    "# Check if SMOTE is applicable\n",
    "if len(np.unique(y_train)) > 1 and np.min(class_counts) > 1:\n",
    "    k_neighbors = min(5, np.min(class_counts) - 1)\n",
    "    if k_neighbors <= 1:\n",
    "        print(f\"Not enough samples for SMOTE with k_neighbors={k_neighbors}\")\n",
    "        X_train_resampled, y_train_resampled = X_train, y_train\n",
    "    else:\n",
    "        try:\n",
    "            smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "            print(\"Training data shape after SMOTE:\", X_train_resampled.shape)\n",
    "        except ValueError as e:\n",
    "            print(\"Error applying SMOTE:\", e)\n",
    "            X_train_resampled, y_train_resampled = X_train, y_train\n",
    "else:\n",
    "    print(\"SMOTE cannot be applied. Check data for class imbalance or size issues.\")\n",
    "    X_train_resampled, y_train_resampled = X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize and train the model with class weights balanced\n",
    "# model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "# model.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model with optimized parameters\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=50,  # Reduced number of trees\n",
    "    max_depth=10,     # Limit depth of trees\n",
    "    max_samples=0.8,  # Use 80% of samples for each tree\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "except MemoryError as e:\n",
    "    print(\"MemoryError while fitting model:\", e)\n",
    "    # Handle error appropriately\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, X_train_resampled):\n",
    "    # Use a subset of the test data if the dataset is very large\n",
    "    sample_size = min(10000, X_test.shape[0])  \n",
    "    X_test_subset = X_test[:sample_size]\n",
    "    y_test_subset = y_test[:sample_size]\n",
    "\n",
    "    try:\n",
    "        # Predict on the test set\n",
    "        y_pred = model.predict(X_test_subset)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test_subset, y_pred)\n",
    "        precision = precision_score(y_test_subset, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test_subset, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test_subset, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(f\"Precision: {precision:.2f}\")\n",
    "        print(f\"Recall: {recall:.2f}\")\n",
    "        print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "        # Anomaly Detection\n",
    "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "        iso_forest.fit(X_train_resampled)\n",
    "\n",
    "        # Predict anomalies\n",
    "        y_anomaly_pred = iso_forest.predict(X_test_subset)\n",
    "        y_anomaly_pred = np.where(y_anomaly_pred == 1, 0, 1)  # Convert -1 to 1 for anomalies\n",
    "\n",
    "        # Evaluate anomaly detection\n",
    "        anomaly_accuracy = accuracy_score(y_test_subset, y_anomaly_pred)\n",
    "        anomaly_precision = precision_score(y_test_subset, y_anomaly_pred, average='weighted', zero_division=0)\n",
    "        anomaly_recall = recall_score(y_test_subset, y_anomaly_pred, average='weighted', zero_division=0)\n",
    "        anomaly_f1 = f1_score(y_test_subset, y_anomaly_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"\\nAnomaly Detection Accuracy: {anomaly_accuracy:.2f}\")\n",
    "        print(f\"Anomaly Detection Precision: {anomaly_precision:.2f}\")\n",
    "        print(f\"Anomaly Detection Recall: {anomaly_recall:.2f}\")\n",
    "        print(f\"Anomaly Detection F1 Score: {anomaly_f1:.2f}\")\n",
    "\n",
    "    except MemoryError as e:\n",
    "        print(\"MemoryError during prediction:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.03\n",
      "Precision: 0.23\n",
      "Recall: 0.03\n",
      "F1 Score: 0.04\n",
      "\n",
      "Anomaly Detection Accuracy: 0.00\n",
      "Anomaly Detection Precision: 0.00\n",
      "Anomaly Detection Recall: 0.00\n",
      "Anomaly Detection F1 Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "if model:\n",
    "    evaluate_model(model, X_test, y_test, X_train_resampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
